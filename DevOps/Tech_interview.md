
<details>
<summary>Linux</summary>
**Q:** Hard link vs soft link<br>
**A:** A hard link is a file all its own, and the file references or points to the exact spot on a hard drive where the Inode stores the data.
A soft link isn't a separate file, it points to the name of the original file, rather than to a spot on the hard drive.
а) Жесткая ссылка не может быть создана для каталогов. Жесткая ссылка может быть создана только для файла.
б) Символические ссылки или симлинки могут ссылаться на каталог.

**Q:** что в Линукс отвечает за открытие файлов?<br>
**A:**
- Inode
inode  или индексный дескриптор — это структура данных в которой хранится метаинформация о стандартных файлах, каталогах или других объектах файловой системы, кроме непосредственно данных и имени.
Ну то есть , грубо говоря, что на 1 файл или директорию тратится 1 inode
На случай если кончились inode, заклинаний не подскажу. Так что для разделов в которы плодятся мелкие файлы следует грамотно выбирать файловую систему или нпйти и удалить все мелкие нулевые файлы.
Пример, мы как-то давно использовали btrfs для хранения множества мелких файлов тк в ней inode создаются динамически
Поискать нулевые файлы



- Как узнать оболочку линукс?
echo $0

Что такое образ initrd?

Начальный RAM-диск (initrd) – это исходная корневая файловая система, которая монтируется до того, как будет доступна настоящая корневая файловая система.
Initrd привязан к ядру и загружен как часть процедуры загрузки ядра.

- SUID SGID Sticky bit
SUID: если установлен бит setuid, когда файл выполняется пользователем, процесс будет иметь те же права, что и владелец исполняемого файла.
SGID: То же  самое что SUID, но только для групп
Sticky bit:  в основном это касается папок, подразумевает, файл или папка, созданные в папке с поддержкой stickybit, могут быть удалены только владельцем. Например , исползование sticky-бита – это /tmp, где каждый пользователь имеет разрешение на запись, но удалить его могут только пользователи, владеющие файлом.

- Состояния процессов:
```
Running (R):
Процесс либо выполняется на процессоре, либо готов к выполнению (ожидает своей очереди).

Sleeping (S или D):
Interruptible sleep (S): Процесс спит (ожидает завершения какого-либо события, например, ввода-вывода) и может быть прерван сигналом.
Uninterruptible sleep (D): Процесс находится в состоянии сна и не может быть прерван сигналом. Обычно это состояние связано с ожиданием завершения операций ввода-вывода.

Stopped (T):
Процесс остановлен, обычно сигналом (например, SIGSTOP или SIGTSTP), и ожидает дальнейших инструкций. Процесс может быть возобновлен с помощью сигнала SIGCONT.

Zombie (Z):
Процесс завершил выполнение, но его запись в таблице процессов ещё не удалена, так как родительский процесс ещё не вызвал wait() для получения статуса завершения. Это состояние временно и длится до того, как родительский процесс обработает завершение дочернего процесса.

Traced (t):
Процесс находится под контролем отладчика (например, gdb).

Idle (I):
В некоторых версиях ядра или инструментов мониторинга может присутствовать это состояние, обозначающее, что процесс ничего не делает, но при этом не находится в ожидании, как в Sleeping.
```

- Что такое zombie процесс?
В Linux (и других Unix-подобных операционных системах) процесс-зомби (zombie process) — это процесс, который завершил своё выполнение, но его запись в таблице процессов всё ещё сохраняется. Это происходит, когда процесс завершает своё выполнение, но его родительский процесс не вызвал системный вызов wait() для получения кода завершения дочернего процесса.

- Основные системные вызовы:
```
- open - открыть файл
- read - пытается читать из файлового дискриптора
- write - пытается записать в файловый дескриптор
- close - закрывает файл после чтения или записи
- fork - создает новый дочерний процесс
- execve - выполняет исполняемый файл
- kill - послать сигнал
```

- Самые популярные сигралы kill
```
15 SIGTERM По умолчанию без параметров. Корректное завершение
9 SIGKILL Немедленное завершение процесса
2 SIGNINT Прерывание процесса (обычно Ctrl + C)
HUP перечиьывает демон или конфигурацию (например, kill HUP PID)

```

Что такое уровни запуска в linux и как их изменить?

Уровень выполнения – это состояние init и всей системы, которое определяет, какие системные службы работают, и они идентифицируются по номерам.

Существует 7 различных уровней выполнения (уровень выполнения 0-6) в системе Linux для различных целей.

```
0: Halt System (To shutdown the system)
1: Single user mode
2: Basic multi user mode without NFS
3: Full multi user mode (text based)
4: unused
5: Multi user mode with Graphical User Interface
6: Reboot System
```

47. Команда TOP
RES (Resident Set Size): Это объем физической памяти (RAM), который используется процессом в данный момент. Включает: Текущие разделяемые библиотеки, а также физическую память, занятую процессом.

VIRT (Virtual Memory Size): Что показывает: Это общий объем виртуальной памяти, используемой процессом. Включает: Все кодовые, данные и разделяемые библиотеки, а также память, выделенную, но не использованную (т.е. не загруженную в RAM).


48. Что такое SeLinux?


SELinux – это аббревиатура от Security-enhanced Linux.

Это реализация контроля доступа и функция безопасности для ядра Linux.
Он предназначен для защиты сервера от неправильной настройки и / или скомпрометированных демонов.

Он устанавливает ограничения и инструктирует серверные демоны или программы, к каким файлам они могут обращаться и какие действия они могут предпринимать, определяя политику безопасности.


49. Какое использование файлов /etc/passwd и /etc/shadow?

/etc/shadow
```
user1:$6$abcd1234$...:19384:0:99999:7:::
```

/etc/passwd
```
user1:x:1000:1000:Иван Иванов:/home/user1:/bin/bash
```


Зачем устанавливать безпарольный вход по ssh?

Чтобы еще больше повысить безопасность системы, большинство организаций решили использовать аутентификацию на основе ключей вместо аутентификации на основе пароля.
Мы можем обеспечить аутентификацию на основе ключей, отключив стандартную аутентификацию по паролю.
Открытый ключ добавляется в файл конфигурации сервера, в то время как личный ключ остается конфиденциальным на стороне клиента.


50. Что такое swappiness в Linux?

Параметр swappiness контролирует стремление ядра перемещать процессы из физической памяти на диск подкачки.


OMM killer -защитный механизм ядра Linux, призванный решать проблемы с нехваткой памяти. При исчерпании доступной памяти он принудительно «убивает» наиболее подходящий по приоритетам процесс, отправляя ему сигнал KILL

Garbage collector - одна из форм автоматического управления памятью. Специальный процесс, называемый сборщиком мусора, периодически освобождает память, удаляя из неё ставшие ненужными объекты.

Load average — это показатель, который используется для отображения средней нагрузки на систему за определенные периоды времени. Он указывает на количество активных процессов, которые либо выполняются, либо ожидают выполнения на процессоре.

51. Как проходит процесс загрузки linux

52. Какие уровни абстрации у LVM

53. Лимиты пользователя в linux ulimit

54. Заголовки TCP пакетов
	SYN
	SYM+ACK
	ACK
	FYN
	WAIT
	RST
	RJCT
	CLOSE

55. Что такое ретраснмишн?
это повторная отправка TCP-сегмента, который не был подтверждён получателем за заданное время.

56. Что такое traceroute и как отрисовывает хосты черех которые прошел пакет?
Основная идея: использовать поле TTL (Time To Live) в IP-заголовке.

Отправляется первый пакет с TTL=1.
Первый маршрутизатор уменьшает TTL до 0 → высылает обратно сообщение ICMP Time Exceeded.
traceroute фиксирует адрес этого маршрутизатора.

Отправляется следующий пакет с TTL=2.
Первый роутер уменьшает TTL → 1, передаёт дальше.
Второй роутер уменьшает TTL → 0, и тоже отвечает ICMP Time Exceeded.
traceroute фиксирует уже второй хоп.

Так продолжается, пока пакет не достигнет целевого хоста.
Когда пакет дойдёт до цели, вместо Time Exceeded придёт ICMP Echo Reply (или UDP/TCP-ответ — зависит от реализации).
Это сигнал, что маршрут найден.

57. Какие виды NAT (SNAT, DNAT)?
SNAT (Source NAT)
Назначение: подмена исходного IP-адреса пакета.
Чаще всего используется, когда хосты в локальной сети (например, 192.168.x.x) выходят в Интернет через один внешний IP.
Пример:
Внутренний хост (192.168.1.10) → NAT → Внешний адрес (203.0.113.5) → Интернет
В Linux реализуется через iptables/nftables (--to-source).

DNAT (Destination NAT)
Назначение: подмена адреса назначения.
Используется для проброса портов (порт-форвардинг), чтобы внешние пакеты попадали к нужному внутреннему серверу.
Пример:
Интернет (1.2.3.4:443) → NAT (203.0.113.5:443 → 192.168.1.100:443) → Внутренний веб-сервер
В Linux: --to-destination.

58.





</details>

<details>
<summary>NGINX</summary>

44. В чем разница между виртуальным хостингом на основе имени и виртуальным хостингом на основе IP?

Виртуальные хосты используются для размещения нескольких доменов на одном экземпляре Apache/Nginx.

Вы можете иметь один виртуальный хост для каждого IP-адреса вашего сервера, или один и тот же IP-адрес, но разные порты, или один и тот же IP-адрес, один и тот же порт, но разные имена хостов.
Последнее называется «ame based vhosts».
На виртуальном хостинге на основе IP мы можем запустить более одного веб-сайта на одном сервере, но каждый веб-сайт имеет свой IP-адрес, в то время как в виртуальном хостинге на основе имен мы размещаем несколько веб-сайтов на одном IP-адресе.
Но для этого вам нужно поместить более одной записи DNS для вашего IP-адреса в базу данных DNS.

45. Как защититься от DDOS / медленных клиентов?
limit_req, limit_conn.
client_body_timeout, client_header_timeout.

45. Конфигурация
/etc/nginx/nginx.conf, sites-enabled/
server {} → виртуальный хост.
location {} → обработка URI.

rewrite vs return?
rewrite → меняет URI и пересылает дальше.
return → сразу возвращает код/ответ (эффективнее для редиректов).

46. Proxy / Load Balancing
Стратегии балансировки:

round-robin (по умолчанию)
least_conn (наименее занятый)
ip_hash (один клиент → один сервер)

Реальный IP клиента
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

47. SSL / Security
HTTPS настройка
```
server {
    listen 443 ssl;
    ssl_certificate /etc/nginx/cert.pem;
    ssl_certificate_key /etc/nginx/key.pem;
}
```

**HSTS** — это механизм безопасности для браузеров:
Говорит браузеру: «Всё, что касается этого домена, нужно использовать только HTTPS»

48. Performance / Тюнинг
worker_processes auto; → по числу CPU.
worker_connections 10240; → макс. соединений на воркер.
sendfile on; → быстрый вывод файлов из ядра.
keepalive_timeout 65; → удержание TCP-сессии.
gzip on; → сжатие.


</details>

<details>
<summary>Monitoring</summary>

## Prometheus vs Zabbix?

| Характеристика       | **Prometheus** | **Zabbix** | **Типичные сценарии использования** |
|----------------------|----------------|-------------|--------------------------------------|
| **Метод сбора**     | Pull (HTTP `/metrics`) | Push/агенты, SNMP, IPMI, JMX | Prometheus в Kubernetes, Zabbix для сетевого оборудования |
| **Архитектура**     | Модульная: Prometheus + Alertmanager + Grafana | Централизованная: сервер + БД + агенты | Prometheus для распределённых систем, Zabbix для дата-центров |
| **Хранение данных** | Временные ряды, хранение недолгое (недели) | Реляционная БД, хранение годами | Prometheus для анализа текущего состояния, Zabbix для долгосрочной статистики |
| **Алертинг**        | PromQL + Alertmanager | Триггеры + действия | Prometheus для гибких правил SLA/SLO, Zabbix для классических алертов (CPU, диск, память) |
| **Визуализация**    | Через Grafana | Встроенные дашборды | Prometheus + Grafana для DevOps-дашбордов, Zabbix для “all-in-one” мониторинга |
| **Масштабируемость**| Легко горизонтально масштабируется (Thanos, VictoriaMetrics) | Зависит от БД, сложнее масштабировать | Prometheus в облаках и микросервисах, Zabbix в корпоративных инфраструктурах |
| **Лучшее применение** | Kubernetes, Docker, cloud-native | Серверы, сети, виртуализация | Prometheus для CI/CD и DevSecOps, Zabbix для ITSM и эксплуатации |
| **Сильные стороны** | Высокая производительность, простая интеграция | Универсальность, поддержка legacy | Prometheus для современных приложений, Zabbix для смешанных сред |
| **Слабые стороны**  | Нет долгосрочного хранения без доп. решений | Тяжёлый, зависимость от SQL-БД | Prometheus требует доп. тулов для полноты, Zabbix сложнее в облаке |

## Архитектура Prometheus
- **Pull-модель**: Prometheus опрашивает таргеты по HTTP `/metrics`.
- **Exporters**: компоненты для экспорта метрик (node_exporter, blackbox_exporter и др.).
- **Pushgateway**: используется для краткоживущих джобов (например, batch-задач).

## Типы данных в Prometheus
1. **Counter** → счётчик (только растёт).
   - Пример: количество HTTP-запросов.
2. **Gauge** → текущее значение (может расти и падать).
   - Пример: загрузка CPU, температура.
3. **Histogram** → распределение значений по "бакетам".
   - Пример: время ответа API.
4. **Summary** → агрегированные квантильные значения.
   - Пример: 95-й перцентиль задержки.

## Labels
- **Labels** — ключ=значение, добавляют контекст к метрикам.
- Пример: `http_requests_total{method="GET", status="200"}`
- Нужны для: фильтрации, агрегации, сравнения метрик между инстансами.

## High Availability (HA)

- Разворачивают несколько Prometheus-инстансов, которые собирают одни и те же метрики.
- Для устранения дублей алертов используют Alertmanager в режиме кластеризации.

## Разница между Alertmanager и Prometheus

- Prometheus → собирает метрики, выполняет запросы, генерирует алерты.
- Alertmanager → управляет алертами (дедупликация, маршрутизация, нотификации в Slack, email, PagerDuty и др.).
</details>

<details>
<summary>Logs</summary>

## Loki vs EFK

| Характеристика | **ELK (Elasticsearch + Logstash + Kibana)** | **Loki** |
|----------------|---------------------------------------------|----------|
| **Индексация** | Индексирует весь текст логов → быстрый поиск, но дорого по ресурсам | Индексирует только **labels** → дешевле хранение, но поиск по тексту медленнее |
| **Хранение**   | Шардирование индексов, высокая нагрузка на storage и CPU | Логи хранятся блобами (**chunks**) в S3, GCS, локальном FS |
| **Интеграция** | Самостоятельная экосистема Elastic | Идеально работает с **Prometheus/Grafana** (общая модель labels) |
| **Стоимость**  | Высокая стоимость эксплуатации (ресурсоёмкость) | Существенно дешевле в эксплуатации |

## Lifehack
Всё, что нужно фильтровать часто → класть в labels.
Всё остальное оставлять в тексте логов.

## Преимущества Loki vs Elasticsearch

Стоимость: дешевле, т.к. индексируются только labels.
Масштабирование: легко хранить petabytes логов в S3.
Интеграция: нативная работа с Prometheus и Grafana.
Простота: не нужны тяжёлые кластеры ES.
Гибкость: легко запускать в Kubernetes (Helm chart, promtail как DaemonSet).

</details>

<details>
<summary>Docker</summary>

Отличие docker  от  VM
В основе VM гипервизор,
Под капотом докера Namespaces + Control groups

namespaces для организации изолированных рабочих пространств, которые мы называем контейнерами.
Список некоторых пространств имен, которые использует docker:

- pid: для изоляции процесса;
- net: для управления сетевыми интерфейсами;
- ipc: для управления IPC ресурсами. (ICP: InterProccess Communication);
- mnt: для управления точками монтирования;
- utc: для изолирования ядра и контроля генерации версий(UTC: Unix timesharing system)


Control groups (контрольные группы)

cgroups для распределение или ограничения ресурсов для  процессов (процессорные, сетевые, ресурсы памяти, ресурсы ввода-вывода)

CMD ENTRYPOINT RUN
CMD sets the command and its parameters to be executed by default after the container is started. However, CMD can be replaced by docker run command line parameters. ENTRYPOINT configures the command to run when the container starts

Инструкции dockerfile:
FROM
ENV
ARG
WORKDIR
RUN
EXPOSE - Никакие порты не открываются, это элемент документирования
VOLUME - внутри контейнера становится томом. При запуске контейнера Docker автоматически создаст том и примонтирует его к этому пути. Данные, записанные в том(volume), сохранятся даже после остановки или удаления контейнера. Например,VOLUME /data.

CMD  явлется как бы дополнением   ENTRYPOINT
те в Entrypoint мы кладем команду ( python server.py),  а в CMD мы кладем  аргуметы к комманде
 Если посмотреть чуть шире, а именно в разрезе  k8s , то  Entrypoint  у нас был зашит в образ, а CMD  мы мы передовали через helm  в зависимости от среды ( dev/qa/stg)  через параметр args

k8s command -> entrypoint
k8s args ->  cmd


Минимизация количества слоев образа


При использовании таких команд, как RUN, COPY, ADD Docker создает слои. Каждый слой увеличивает размер образа, так как слои кэшируются.
Чтобы уменьшить количество слоев, необходимо объединять (комбинировать) команды в цепочки для того, чтобы исключить проблемы, связанные с неправильным использованием кэша. Рассмотрим эти рекомендации на конкретных примерах. Предположим, нам необходимо выполнить следующие 2 команды:
```
RUN apt update
RUN apt -y install tree
```
Если вы используете apt, необходимо комбинировать в одной инструкции RUN команды apt update и apt install. Команды выше необходимо скомбинировать в одну команду следующим методом:
```
RUN apt update && apt -y install tree
```
В результате вместо двух слоев будет создан один слой, и как итог будет уменьшен размер финального образа. Кроме того, следует объединять в одну инструкцию команды установки пакетов. Перечислять пакеты необходимо на нескольких строках, разделяя список символами \. Выглядеть это может так:
```
RUN apt update && apt install -y \
	htop \
	tree \
	mc
```
Этот метод также позволяет сократить число слоёв, которые должны быть добавлены в образ, и помогает поддерживать код файла в читаемом виде.

Удаление кэшей и временных файлов

При использовании пакетных менеджеров, таких как apt, apk, yum/dnf, они кэшируют загружаемые данные с целью снижения нагрузки на сеть, и, как следствие, уменьшается время, требуемое для установки программ. Данный кэш необходимо удалять, чтобы размер итогового образа не разрастался до больших объемов.
Для удаления кэша в конец команды по установке (например, apt install) необходимо добавить одну из нижеперечисленных строк — в зависимости от используемого пакетного менеджера:
```
APT: ... && rm -rf /var/cache/apt
APK: ... && rm -rf /etc/apk/cache
YUM: ... && rm -rf /var/cache/yum
DNF: ... && rm -rf /var/cache/dnf
```

Cетевые драйверы docker
несколько драйверов по умолчанию, которые обеспечивают основной функционал по работе с сетью:
- none: отключение всех сетевых ресурсов.
- bridge: сетевой драйвер по умолчанию. По сути, это мост между контейнером и хостовой машиной. Мостовые сети обычно используются, когда приложения выполняются в автономных контейнерах, которые должны взаимодействовать друг с другом.
- host: для автономных контейнеров устраняется сетевая изолированность между контейнером и хостом Docker и напрямую используются сетевые ресурсы хоста.
- overlay: наложенные сети соединяют несколько демонов Docker.
- macvlan: сети Macvlan позволяют присваивать контейнеру MAC-адрес, благодаря чему он выглядит как физическое устройство в сети.

Docker многостадийная сборка (multi-stage build) — это методика сборки образов Docker, которая позволяет создавать более легкие и оптимизированные образы с помощью нескольких этапов сборки в одном Dockerfile. Это особенно полезно для создания минимальных продакшн образов, содержащих только необходимое программное обеспечение и зависимости, без лишних файлов, которые были нужны только на этапе сборки или тестирования.

Как уменьшить размер docker образа?
	1. Выбор базового образа, например alpine
	2. Минималтзвация установленных зависимостей
	3. Multi-stage build
	4. Почистить каталог /var
	5. Минималищировать количесво слоев
		Слой = snapshot файловой системы после выполнения инструкции.
		Docker использует UnionFS (OverlayFS/aufs) для объединения слоёв.

</details>

<details>
<summary>Kubernetes</summary>

Компоненты:

Kube-api - предоставляет kubernets API
ETCD - Хранит все данные о состоянии кластера (конфигурация, секреты, статусы и т.п.).
Sheduler - разпрелеляет приложения, назначает рабочий узел каждому приложению
kube-controller-manager -
Управляет контроллерами (controllers), которые следят за текущим состоянием кластера и стремятся к желаемому состоянию.
Примеры:
	ReplicaSet Controller — гарантирует нужное количество реплик Pod'ов.
	Node Controller — следит за состоянием узлов.
	Deployment Controller — управляет обновлениями приложений.

kubelet
Агент, который получает инструкции от API Server и управляет контейнерами на узле.


kube-proxy
kube-proxy — сетевой прокси, работающий на каждом узле в кластере, и реализующий часть концепции сервис.


IRSA (AWS only) - this option creates an identity provider for the cluster.
After this step we can add role and policy on the side of Amazon cloud, I mean IAM service.
Add the identity number of this role to the k8s service account as annotation and assign this service account to the appropriate pod or deployment.

Role —  который описывает некий набор прав на объекты кластера Kubernetes. Role ничего и никому не разрешает. Это просто список.
RoleBinding -
ServiceAccount -
Пример: kubectl auth can-i get pods --as <serviceaccount> -n <namespace>

1. Liveness Probe используется для определения необходимости перезапуска контейнера.
2. Readiness Probe используется для определения готовности контейнера принимать трафик, ну то есть когда пускать/не пускать трафик.
3. Startup Probe используется для проверки успешного завершения инициализации контейнера и позволяет игнорировать liveness и readiness проверки до её завершения.

Pod Disruption Budget (PDB)
```
это механизм в Kubernetes, который помогает управлять количеством pod'ов (единиц приложения) в кластере, которые могут быть одновременно остановлены или перезапущены. PDB гарантирует, что определенное количество pod'ов останется доступным даже в случае непредвиденных сбоев или плановых обновлений.
```
QoS (Quality of Service) class
```
метрика определяющая приоритет пода (Pod) в кластере на основе его потребностей в ресурсах (CPU и памяти).
Guaranteed (Гарантированный) -  самый высокий приоритет (request == limits)
Burstable (Гибкий) - Они более гибки, чем Guaranteed, и их могут удалить раньше (requests < limits)
Best-Effort (Лучший вариант) - (Ни requests, ни limits) поды будут удалены первыми при нехватке ресурсов.
```

NodeSelector - юнит в описании манифеста, который служит для выбора нод под запуск подов ( например нода для запуска с меткой GPU)
Affinity - используется для размещение подов на соответствущих описанным критериям нодах при определнных условиях.( например запускать поды из данного деплоймента на нодах из определенной availibility zone,  instance type, etc)
AnitAffinity -  для обратного вышеописанному
taint -   для того чтобы запретить подам быть запущенным на определнных нодах. Использается при описании  NodeGroup(Ноды)

Чтобы поды от деплоймента в Kubernetes распределялись на разные ноды, а не на одну, можно воспользоваться несколькими подходами:

 Использование PodAntiAffinity

PodAntiAffinity позволяет указывать, что поды не должны размещаться на одной и той же ноде. Это настраивается с помощью полей affinity и podAntiAffinity в манифесте деплоймента.

Как работает OOM Killer в Kubernetes
```
Лимиты и запросы памяти:

В Kubernetes ресурсы, такие как память и CPU, управляются с помощью параметров requests и limits, которые можно задавать в манифесте контейнера.
requests — это минимальное количество ресурсов, которое гарантированно выделяется контейнеру.
limits — это максимальное количество ресурсов, которое контейнер может использовать.
Переполнение лимитов памяти:

Если контейнер начинает использовать больше памяти, чем указано в limits, ядро Linux может сработать и активировать OOM Killer.
Когда OOM Killer активируется, он завершает процессы, чтобы освободить память. В контексте контейнеров это означает завершение всего контейнера.
Выбор процесса для завершения:

OOM Killer выбирает процессы для завершения на основе их "oom_score", который рассчитывается ядром на основании различных факторов, таких как размер процесса, время его выполнения и важность.
Контейнеры, которые потребляют больше памяти, чем указано в limits, получают повышенный "oom_score" и, следовательно, становятся приоритетными кандидатами для завершения.
Последствия в Kubernetes:

Когда контейнер убивается OOM Killer-ом, Kubernetes помечает этот контейнер как OOMKilled и может попытаться перезапустить его, в зависимости от настроек restartPolicy.
Если контейнер постоянно превышает лимит памяти и OOM Killer регулярно его завершает, это может привести к циклическим перезапускам (crash loop).
События и диагностика:

Kubernetes генерирует событие, указывающее, что контейнер был завершен из-за превышения лимита памяти (событие типа OOMKilled).
Для диагностики можно просмотреть логи пода и события с помощью команд kubectl logs и kubectl describe pod.
Профилактика:

Чтобы избежать ситуаций с OOM Killer, рекомендуется правильно рассчитывать requests и limits для контейнеров на основе профилирования и мониторинга приложения.
Можно также использовать инструменты мониторинга, такие как Prometheus, для наблюдения за потреблением памяти и другими метриками в кластере.
Таким образом, OOM Killer играет важную роль в управлении памятью и обеспечении стабильности узлов в Kubernetes, но требует внимательной настройки ресурсов для контейнеров, чтобы избежать нежелательных завершений и перезапусков.
```
**Q** - Throtling in Kubernetes:
**A** - это механизм ограничения количества процессорного времени (CPU), доступного контейнеру, когда он превышает установленные для него лимиты (CPU limits)

**Q** - Как работает throtling в k8s:
**A** <br>
1. Настройка лимитов CPU:<br>
Пользователь устанавливает лимиты для CPU в конфигурации Pod'а.<br>
2. Превышение лимита:<br>
Когда приложение в контейнере пытается использовать больше CPU, чем выделено, механизм троттлинга активируется.<br>
3. Пропуск тактов:<br>
Вместо предоставления полного доступа к ресурсам, ядро Linux (с использованием cgroups) ограничивает количество машинных циклов (тактов), которые получает контейнер.<br>
4. Замедление работы:<br>
В результате приложение начинает работать медленнее, так как ему не хватает процессорного времени для выполнения всех операций.<br>


В Kubernetes LimitRange и ResourceQuota — это механизмы, которые позволяют ограничивать использование ресурсов (CPU, память) в namespace'ах , чтобы предотвратить чрезмерное потребление ресурсов и обеспечить стабильность кластера.

LimitRange - Ограничивает минимальные и максимальные значения requests и limits на уровне отдельного контейнера или Pod'а внутри namespace.

ResourceQuota - Ограничивает общее использование ресурсов в namespace . То есть, сколько всего может быть выделено ресурсов всем Pod'ам в рамках одного namespace.

Какие виды контейнеров бывают в поде?
	- init
	- sidecar

statefullset VS deployment?


Типы сервисов в k8s(4):
-ClusterIP
-NodePort
-LoadBalancer

Какие механизмы безопасности есть в k8s?
	RBAC
	Network Policies
	SecurityContext — настройка прав пода/контейнера (uid/gid, readOnlyRootFilesystem, drop capabilities).
	Pod Security Standards (PSS, пришли на смену PodSecurityPolicy) - запреты на запуск подов с root-правами, привилегированными capability, hostPID/hostNetwork и т.д.

Что произрйдет с контейнером если он превысит потребление CPU?

Что такое ingress?

В чем преимущество использования ингресса против сервиса при обращение прилодений в рамках одного кластера к друг другу?



**Questions**
- Какие вопросы вы зададите разработчику, когда он приносит код для деплоя в Kubernetes?
- Какие kubernetes-объекты используете для деплоя stateful приложения?
- Как диагностировать задержки между двумя кластерами с раздельными БД и приложениями?
```sh
У каждого приложения есть своя база ранных. Эти приложенияя общаются через интеренет по https. С первого кластера приложение отправляяет во второе приложение, там приложение записывает в базу, отсылает ответ и первый записывает в свою базу, Этот раунд длятся 400 милисекунд, но бывают задержки 3 секунды. Куда смотреть?
```
- Какие ресурсы проверяете на хосте для обеспечения надёжной работы БД?
- Какие шаги предпримете при оптимизации БД, например Postgres?
- Как анализируете сетевые проблемы между двумя кластерами?

</details>

<details>
<summary>Terraform</summary>

## 1️⃣ Общие вопросы

**Q:** Что такое Terraform?<br>
**A:** IaC-инструмент для создания, изменения и управления инфраструктурой безопасно и предсказуемо.

**Q:** Terraform vs Ansible/Chef/Puppet?<br>
**A:** Terraform — управление инфраструктурой; Ansible/Puppet — конфигурация существующих ресурсов.

**Q:** Что такое state-файл (`terraform.tfstate`)?<br>
**A:** Хранит текущее состояние ресурсов, позволяет Terraform понять, что создавать, менять или удалять.

**Q:** Разница между `plan` и `apply`?<br>
**A:** `plan` — показывает изменения; `apply` — применяет их.

---

## 2️⃣ Ресурсы и зависимости

**Q:** Что такое resource, data source, module?<br>
**A:**
- `resource` — создаёт объект инфраструктуры.
- `data source` — получает существующие данные.
- `module` — переиспользуемый блок конфигурации.

**Q:** Как управлять зависимостями между ресурсами?<br>
**A:** Через `depends_on` или использование атрибутов одного ресурса в другом.

**Q:** Что такое provisioners?<br>
**A:** Скрипты, выполняемые после создания ресурса (`remote-exec`, `local-exec`). Рекомендуется использовать редко.

---

## 3️⃣ State и Backend

**Q:** Типы backend?<br>
**A:** `local` (локальный), `remote` (S3, GCS, Terraform Cloud, Azure Storage, Consul).

**Q:** Зачем нужен locking?<br>
**A:** Чтобы несколько человек/процессов не меняли состояние одновременно.

**Q:** Как хранить секреты в Terraform?<br>
**A:** Через env variables (`TF_VAR_...`), Vault, AWS Secrets Manager, SSM Parameter Store.

---

## 4️⃣ Модули

**Q:** Что такое Terraform Module?<br>
**A:** Переиспользуемая конфигурация, локальная или из registry.

**Q:** Разница root vs child module?<br>
**A:** Root — основной конфиг; Child — импортируется в root.

**Q:** Как передавать переменные в module?<br>
**A:** Через `variables` в module и `module.<name>.<var>` в root.

---

## 5️⃣ Переменные и output

**Q:** Типы переменных?<br>
**A:** string, number, bool, list, map, set, object, tuple.

**Q:** Зачем output?<br>
**A:** Возвращает значения из module или конфигурации для использования в других ресурсах или CI/CD.

---

## 6️⃣ Провайдеры и версии

**Q:** Что такое provider?<br>
**A:** Плагин, позволяющий Terraform работать с конкретной платформой (AWS, Azure, GCP, Kubernetes).

**Q:** Как ограничить версию Terraform и провайдера?<br>
**A:** В `required_version` и `required_providers`.

---

## 7️⃣ Работа с инфраструктурой

**Q:** Что такое immutable infrastructure?<br>
**A:** Каждое обновление создаёт новые ресурсы вместо изменения существующих → минимизация downtime.

**Q:** `taint` и `import`?<br>
**A:**
- `terraform taint <resource>` — пометить ресурс на пересоздание.
- `terraform import` — импорт существующего ресурса в state.

---

## 8️⃣ Advanced / практические

**Q:** Разница между `count`, `for_each`, `dynamic block`?<br>
**A:**
- `count` — повторение ресурса N раз.
- `for_each` — создание ресурсов на основе списка или карты с ключами.
- `dynamic block` — динамическая генерация вложенных блоков.

**Q:** Как защитить ресурсы от удаления?<br>
**A:** `lifecycle { prevent_destroy = true }`, `ignore_changes` для атрибутов.

**Q:** CI/CD с Terraform?<br>
**A:** `plan` → ревью → `apply`. Через GitHub Actions, GitLab CI/CD, Terraform Cloud/Enterprise.

**Q:** Отладка ошибок?<br>
**A:** `TF_LOG=DEBUG terraform apply`, `terraform validate`, `terraform fmt`, проверка state и зависимостей.

**Q:** Что такое null ресурс?<br>
**A:** null_resource является ресурсом , который позволяет настроить provisioners, которые непосредственно не связаны с одним существующим ресурсом.

## 9️⃣ Полезные функции Terraform

- `file(path)` — считывает файл
- `jsondecode(string)` / `jsonencode(obj)` — JSON ↔ объект
- `base64encode/decode(string)` — кодирование/декодирование
- `coalesce(a,b,...)` — первый ненулевой аргумент
- `concat(list1,list2)` — объединение списков
- `length(list/map/string)` — длина



</details>

<details>
<summary>Брокеры</summary>

smth

</details>
<details>
<summary>Базы данных</summary>

smth

</details>

<details>
<summary>AWS</summary>


AWS System manager - is a secure end-to-end management solution for resources on AWS and in multi-cloud and hybrid environments

Availability zone
- Region (регион) – это географическая область (например, us-east-1 – Вирджиния, США).
- Availability Zone (зона доступности) – это один из дата-центров внутри региона, имеющий независимое электропитание, сеть и охлаждение.
- В каждом регионе обычно есть минимум 2–3 зоны доступности (например: us-east-1a, us-east-1b, us-east-1c).

VPC - виртуальная частная сеть или изолированный сегмент

- Nat gateway - Позволяет ресурсам из private subnet выходить в интернет только для исходящего трафика

- Internet gateway - Позволяет ресурсам с публичным IP адресом принимать входящие соединения, наприемр Elastic LB. Оплата только за трафик.

- Security groups - firewall at the instance level
acts as a firewall that controls the traffic allowed to and from the resources in your virtual private cloud (VPC). You can choose the ports and protocols to allow for inbound traffic and for outbound traffic.

- Security groups are stateful. For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.

network ACL  - Список управления доступом к сети (ACL) разрешает или запрещает определенный входящий или исходящий трафик на уровне подсети.
	Default network ACL - allows all inbound traffic
	Custom network ACL - denies all inbound and outbound traffic untill you add rules
	Block specific IP addresses^ not security group

Site-to-Site VPN -  connection between remote devices and AWS resources. Virtual Private Gateway --> IPSEC --> Customer gateway (on-premise network)
Network ACLs are stateless, which means that return traffic must be explicitly allowed by the rules.

- External Ingress

Доступен из интернета.
Для него создаётся Internet-facing Load Balancer.

Используется, когда нужно отдавать сервисы/приложения наружу (например, API, веб-приложение).

В AWS ALB это означает:
scheme: internet-facing
Security Group разрешает входящий трафик с 0.0.0.0/0 (или с ограничений).
Пример аннотации для Kubernetes Ingress:

```
annotations:
  alb.ingress.kubernetes.io/scheme: internet-facing
```

2. Internal Ingress

Доступен только внутри VPC (или через VPN/Direct Connect/PrivateLink).
Для него создаётся Internal Load Balancer.
Используется для внутренних микросервисов, админских панелей, сервисов, к которым не нужен публичный доступ.

В AWS ALB это означает:

scheme: internal

Security Group обычно ограничивает доступ только из корпоративных сетей или из подсетей VPC.

Пример аннотации:

```
annotations:
  alb.ingress.kubernetes.io/scheme: internal
```


Direct connect - directly connect to AWS data center without internet

AWS Private link - Establish connectivity between VPCs and AWS services without exposing data to the internet

Athena -  сервис запросов, похожий на SQL

Amazon OpenSearch Service makes it easy for you to perform interactive log analytics,

VPC Flow logs - Опция для логирования сетевого трафика

CloudFront- system of distributed servers that deliver webpages and other web content

Application Load Balancer принимает решения о маршрутизации на уровне приложения (HTTP/HTTPS), поддерживает маршрутизацию на основе пути и может направлять запросы на один или несколько портов в каждом экземпляре контейнера в вашем кластере.

Network Load Balancer  принимает решения о маршрутизации на транспортном уровне (TCP/SSL). Он может обрабатывать миллионы запросов в секунду. После того, как балансировщик нагрузки получает соединение, он выбирает цель из целевой группы для правила по умолчанию, используя алгоритм маршрутизации хеширования потока. Он пытается открыть TCP-соединение с выбранной целью на порту, указанном в конфигурации прослушивателя. Он пересылает запрос без изменения заголовков.

Route53
	Simple - Rote traffic to a single resource
	Failover - Active-passive failover
	Geolocation - Route traffic based on the location
	Geoproximity - Based on the physical distance between your users and your resources
	Latency based - Route traffic based on the based latency to provide good performance
	Multivalue Answers - Enable Route53 to respond with up to end to eight selected at random (Round-robin)
	Weighted - Route traffic to multiple resources based on a numerical weight

VPC
VPC -->  add subnets --> create Rote table --> create NACL --> associate subnets with route tables

VPC endpoint with acces to S3
Launch EC2 --> Create Vpc endpoint for S3 --> Review route table

S3
Standard - >= 3 AZs    99.9 % (For workloads and frequent data)
Standard infrequent access -  >= 3 AZ Long-term infriquent accessed critical data
One Zone Infrequent access - 1 AZ Long term infriquent access, non-critical
Glacier Instant Retrieval - >=3 AZ Long live data. For infrequent data (Minimum duration 90 days)
Glacier Flexible Retrieval >=3 AZ Long-term data archiving needs to be accessed within a few hours or minutes
Glacier deep archive - >=3 AZ Rerely accessed data archiving. default retrieval time 12 hours (Minimum duration 180 days)
S3 Intelligent-tiering - >=3 AZ Unknown or unpredictable access pattern

Server-side encryption:
	SSE-S3 - S3 managed keys AES 256-bit encryption
	SSE-KMS - AWS Key Management Service managed keys
	SSE-C - Customer-provided keys

S3 Static website
	Enable static website -> Disable Block Public Access settings -> Allow public read access for your objects

Inventory bucket - Used to help understand how you are storing objects in S3 bucket

EFS storage classes:


AWS Config continually assesses, audits, and evaluates the configurations and relationships of your resources on AWS,

File gateway - Access files stored on S3 using NFS or SMB
Fsx Gateway - Access files in Amazon Fsx for Windows File Server using SMB
Volume gateway(Stored Mode) - Your entire dataset is stored on-site and backend up to S3 as RBS snapshots.
Volume Gateway (Cached mode) - Your entire dataset is stored in S3 (only frequently accessed)


1 subnet in 1 availability zone

security group - фильтрует трафик на уровне инстанса
NACL - фильтркет трафик на уровне подсети

---

AWS cloudwatch
- Logs
- insights
- Metrics
- Log stream
- Log groups
- Log archival

может пересылать логи в S3 Opensearch Lambda kinesis

AWS Cloudtrail - сервис для аудит API вызовыв

X-RAY -

Network Monitor -

</details>


<details>
<details>
<summary>Cloudflare</summary>

smth

</details>
<summary>Виды тестов</summary>
виды тестов:
Юнит-тестирование (Unit Testing):
Тестирование отдельных компонентов или модулей приложения изолированно.
Цель: проверить, что каждый модуль работает правильно.

Интеграционное тестирование (Integration Testing):
Тестирование взаимодействия между модулями или компонентами.
Цель: выявить проблемы, которые могут возникнуть при взаимодействии компонентов.

Функциональное тестирование (Functional Testing):
Проверка функциональности приложения в соответствии с требованиями.
Фокусируется на том, что система делает.

Системное тестирование (System Testing):
	Комплексное тестирование всей системы целиком.
	Цель: убедиться, что приложение соответствует всем заявленным требованиям и спецификациям.

Приёмочное тестирование (Acceptance Testing):
	Тестирование, проводимое для подтверждения того, что система удовлетворяет потребности пользователя.
	Включает альфа- и бета-тестирование.

Регрессионное тестирование (Regression Testing):
	Повторное тестирование системы для проверки, что изменения в коде не вызвали новых дефектов.
	Обычно проводится после внесения исправлений или новых функций.

Нагрузочное тестирование (Load Testing):
Тестирование производительности системы под различными уровнями нагрузки.
Цель: определить, как система работает при увеличении числа пользователей или объема данных.

Стресс-тестирование (Stress Testing):
Тестирование системы под экстремальными условиями, превышающими нормальные рабочие нагрузки.
Цель: выявить пределы прочности и стабильности системы.

Тестирование производительности (Performance Testing):
Оценка быстродействия системы в различных условиях.
Включает измерение времени отклика, пропускной способности и использования ресурсов.

Тестирование безопасности (Security Testing):
Проверка системы на уязвимости и угрозы безопасности.
Цель: защитить данные и обеспечить безопасную работу системы.

Тестирование удобства использования (Usability Testing):
Оценка удобства и интуитивности интерфейса для конечных пользователей.
Включает сбор отзывов пользователей и наблюдение за их взаимодействием с системой.

Тестирование совместимости (Compatibility Testing):
Проверка работы приложения на различных платформах, устройствах и браузерах.
Цель: убедиться, что система работает корректно в различных средах.

Тестирование восстановления (Recovery Testing):
Оценка способности системы восстанавливаться после сбоев и отказов.
Цель: убедиться, что система может восстановиться и продолжить работу после критических ошибок.

Тестирование локализации (Localization Testing):
Проверка адаптации приложения для различных регионов и языков.
Включает проверку правильности перевода, форматов даты и времени, валют и других региональных особенностей.

</details>

<details>
<summary>Иные вопросы</summary>

**Q** - Что такое 12-Factor App?<br>
**A** - 12-Factor App — это методология разработки облачных веб-приложений, которая задаёт 12 принципов для создания масштабируемых, управляемых и надежных сервисов. Основная идея — приложения должны быть:
полностью конфигурируемыми через переменные окружения,
без состояния в процессе, чтобы масштабировать их горизонтально,
изолированными от внешних сервисов, которые могут быть заменены без изменений кода,
совместимыми с CI/CD и контейнеризацией, что упрощает развертывание, откат и автоматизацию.


REST - архитектурный стиль. Любой формат (JSON, XNL etc)
SOAP - протокол обмена структурированными данными. Протокол (SOAP XML)


Чем отличаются протоколы http1.1 от http2.0 ?

Что такое треды операционной системы?

CORS - служит для того, чтобы например регуоировать общение браузера с апишками через AJAX запрос

**Q** - Что происход при curl ifconfig.io
**A** - curl вызывает функции ОС (getaddrinfo), чтобы найти IP-адрес домена.
ОС обращается к /etc/hosts и DNS-серверам (например, 8.8.8.8 или провайдера).
В итоге ya.ru превращается в IP (например, 77.88.55.242).

🔹 3. Установление TCP-соединения
curl открывает сокет и делает TCP 3-way handshake:
SYN → клиент → сервер
SYN+ACK ← сервер
ACK → клиент
Теперь установлен TCP-канал (обычно к :80 или :443).

🔹 4. TLS-рукопожатие (если HTTPS)
Если вызвать curl https://ya.ru, произойдёт:
обмен ключами (TLS handshake),
проверка сертификата сайта,
установка шифрованного канала.

🔹 5. Отправка HTTP-запроса
curl формирует минимальный запрос:
GET / HTTP/1.1
Host: ya.ru
User-Agent: curl/8.x
Accept: */*


GET / — просим корневую страницу.
Host: ya.ru — нужен для виртуального хостинга.

🔹 6. Обработка на сервере

Запрос попадает в инфраструктуру Яндекса (балансировщики, прокси, веб-сервер).
Сервер подготавливает ответ (чаще всего — редирект на https://ya.ru/).

🔹 7. Ответ сервера

Пример ответа:
HTTP/1.1 301 Moved Permanently
Location: https://ya.ru/
Content-Length: 0
Connection: keep-alive


Код 301 говорит: «ресурс навсегда переехал на HTTPS».
Заголовок Location указывает новый адрес.

🔹 8. Обработка ответа curl

По умолчанию curl просто выведет тело ответа (в данном случае пустое).
Заголовки можно увидеть с флагом -v или -I.

✅ Итог: при curl ya.ru происходит:

DNS-резолвинг имени → IP.
Установление TCP-соединения.
Установление TLS-рукопожатие.
Отправка HTTP-запроса.
Получение и вывод HTTP-ответа (обычно редирект).
</details>

<details>
<summary>PKI and TLS</summary>

smth

</details>

<details>
<summary>Мой последний проект</summary>

                           Users
                             │
                             ▼
                        +------------+
                        | Cloudflare |
                        +------------+
                             │  DNS/WAF/DDoS
                             ▼
                    +---------------------+
                    |   ALB (Ingress)     |
                    | Public Subnets (AZ) |
                    +---------------------+
                      /          |         \
                     /           |          \
                    ▼            ▼           ▼
           ┌─────────────┐ ┌─────────────┐ ┌─────────────┐
           |  Private    | |  Private    | |  Private    |
           |  Subnet AZ1 | |  Subnet AZ2 | |  Subnet AZ3 |
           └─────────────┘ └─────────────┘ └─────────────┘
                  │              │               │
        +----------------+ +----------------+ +----------------+
        |  EKS Nodes     | |  EKS Nodes     | |  EKS Nodes     |
        |  (Worker pool) | |  (Worker pool) | |  (Worker pool) |
        +----------------+ +----------------+ +----------------+
           |      |          |      |           |      |
           ▼      ▼          ▼      ▼           ▼      ▼
        +--------------------------------------------------+
        |                  EKS Cluster                     |
        |   (Pods, Deployments, Services, DaemonSets, etc) |
        +--------------------------------------------------+

	┌─────────────────────────────────────────────────────────┐
	│                   AWS Networking Layer                  │
	│                                                         │
	│  VPC 10.0.0.0/16                                        │
	│   • Public Subnets (IGW) → ALB                          │
	│   • Private Subnets (NAT GW) → Worker Nodes             │
	│   • Route Tables:                                       │
	│       Public RT → Internet Gateway                      │
	│       Private RT → NAT Gateway                          │
	└─────────────────────────────────────────────────────────┘


Сетевой фундамент (VPC, сабнеты, маршруты)
- VPC

CIDR: например, 10.0.0.0/16.

Разделение на 3 AZ (для отказоустойчивости).

Сабнеты

Public subnets (по одной в каждой AZ)

для ingress, bastion-хостов, NAT Gateways.
ассоциируем с route table → Internet Gateway.

Private subnets (по одной в каждой AZ)
для воркеров EKS.

ассоциируем с route table → NAT Gateway (чтобы узлы могли тянуть апдейты, но не были доступны извне).
Route Tables

Public RT → IGW.
Private RT → NAT GW.

Security Groups

Для EKS Control Plane (AWS управляет, но мы должны разрешить вход с нод).

Для worker nodes: 443 (kubelet), NodePort (если нужен), ограниченный доступ между подсетями.

- EKS через Terraform

Создание кластера
Используем модуль terraform-aws-eks.

Указываем VPC ID
Указываем private subnets для нод(важно: control plane общается с воркерами через них).
Control Plane — managed by AWS (highly available across AZ).
Worker Nodes
Managed Node Groups (обычно on-demand).
Spot Node Group для дешёвых воркеров (CI/CD, не-критичные поды).

Taints/Tolerations — чтобы разделить workload (например, критика vs. batch).
Networking для кластера

CNI: AWS VPC CNI (каждый pod получает IP из сабнета).
Поддержка pod density зависит от выбранных типов инстансов и /26-/28 блоков в сабнетах.
Можно добавить Calico, если нужна NetworkPolicy.

- IAM + RBAC

OIDC provider для EKS (нужен для IRSA — IAM Role for ServiceAccount).
Чтобы сервис-аккаунты могли получать IAM роли (IRSA).
Это позволяет подам напрямую работать с AWS сервисами (S3, Secrets Manager, DynamoDB и т.д.).
Привязываем роли к k8s сервис-аккаунтам (например, pod в k8s может читать S3 или секреты в Secrets Manager).
Add-ons (сразу через Terraform или Helm)
vpc-cni
coredns

kube-proxy

ingress controller (nginx или AWS Load Balancer Controller).

- Поток трафика (пример)

Пользователь → Cloudflare (DNS -> WAF -> DDoS).

Cloudflare → ALB (ingress) в public subnet.

ALB → EKS pods (через сервис типа LoadBalancer/Ingress).

Для админки был выбрал ingress типа internal

Pods сидят в private subnet, имеют выход в интернет через NAT Gateway (например, для скачивания образов).

На собеседовании можно упомянуть

Зачем делаем private-only worker nodes (безопасность).

Почему используем несколько сабнетов в разных AZ (HA).

Как масштабируем кластер: Cluster Autoscaler + разные Node Groups (on-demand/spot).

Как обеспечиваем доступ: kubectl через bastion или через AWS SSM Session Manager, а не открытый 22 порт.
</details>